<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Briefs — Tony Arteaga</title>
  <meta name="description" content="Short takes on AI, tools, and what I'm learning." />
  <link rel="icon" type="image/png" href="assets/favicon-96x96.png" sizes="96x96" />
  <link rel="icon" type="image/svg+xml" href="assets/favicon.svg" />
  <link rel="shortcut icon" href="assets/favicon.ico" />
  <link rel="apple-touch-icon" sizes="180x180" href="assets/apple-touch-icon.png" />
  <link rel="manifest" href="site.webmanifest" />
  <link rel="canonical" href="https://tony-art.com/briefs.html">
  <script src="https://cdn.tailwindcss.com"></script>
</head>
<body class="bg-slate-50 text-slate-900 antialiased">
  <!-- Sticky nav -->
  <div class="sticky top-0 z-50 bg-slate-50/95 backdrop-blur-sm border-b border-slate-200">
    <div class="max-w-4xl mx-auto px-6 py-4">
      <nav class="flex justify-between items-center">
        <a href="index.html" class="text-xl font-semibold hover:underline">Tony Arteaga</a>
        <div class="space-x-4 text-sm">
          <a href="index.html" class="hover:underline">Home</a>
          <a href="about.html" class="hover:underline">About</a>
          <a href="ai-journey.html" class="hover:underline">AI Journey</a>
          <a href="briefs.html" class="underline font-medium">Briefs</a>
          <a href="index.html#contact" class="hover:underline">Contact</a>
          <a href="resume.html" class="inline-block rounded px-3 py-1 bg-slate-900 text-white">Resume</a>
        </div>
      </nav>
    </div>
  </div>

  <main class="max-w-4xl mx-auto px-6 pb-20">
    <section class="mt-4">
      <h1 class="text-3xl font-bold">Briefs</h1>
      <p class="mt-2 text-slate-600">Short takes on AI, tools, and other relevant business news.</p>
    </section>

    <!-- Brief: 2028 scenario -->
    <article class="mt-12 border-b border-slate-200 pb-10">
      <time class="text-sm text-slate-500">February 24, 2026</time>
      <h2 class="text-xl font-semibold mt-1">The 2028 scenario everyone should read — and why Anthropic takes it seriously</h2>
      <div class="mt-3 text-slate-700 space-y-3">
        <p>
          A research piece by Citrini Research went viral this week — 1,700 likes, 351 restacks on Substack, enough to spook markets on Monday (Dow -1.7%). The WSJ picked it up. Here's what it actually says, and why it matters to me.
        </p>
        <p>
          The piece is a thought exercise, explicitly not a prediction. It's written as a June 2028 macro memo looking back at how things unraveled. The scenario in brief: in late 2025, agentic coding tools — the paper specifically names Claude Code and Codex — took a step-function jump in capability. By mid-2026, enterprise procurement teams had seen enough demos to ask a dangerous question: <em>"What if we just built this ourselves?"</em> The long-tail of SaaS started collapsing. Companies most threatened by AI became AI's most aggressive adopters — not out of vision, but survival. Margins expanded. Stocks rallied. Profits funneled back into AI compute.
        </p>
        <p>
          The problem was what happened next. White-collar workers who lost jobs didn't go back to spending. The consumer economy — 70% of GDP — withered. What the paper calls "Ghost GDP" emerged: productivity gains that showed up in national accounts but never circulated through the real economy. A negative feedback loop with no natural brake.
        </p>
        <p>
          The paper closes: <em>"But you're not reading this in June 2028. You're reading it in February 2026. The S&P is near all-time highs. The negative feedback loops have not begun... As a society, we still have time to be proactive. The canary is still alive."</em>
        </p>
        <p>
          I believe this scenario is plausible. More importantly, I believe most people in a position to do something about it aren't saying so out loud. Dario Amodei has — openly, repeatedly. He's talked about AI potentially compressing decades of scientific progress into years, and in the same breath about the obligation that creates. That honesty is rare, and it's a big part of why I want to be at Anthropic.
        </p>
        <p>
          <em>The canary is still alive. That's the window.</em>
        </p>
      </div>
      <p class="mt-4 text-sm text-slate-500">
        Sources: <a href="https://www.citriniresearch.com" class="underline hover:text-slate-700">Citrini Research</a> · "The 2028 Global Intelligence Crisis" · Feb 22, 2026 &nbsp;|&nbsp;
        <a href="https://www.wsj.com" class="underline hover:text-slate-700">The Wall Street Journal</a> · Feb 24, 2026
      </p>
    </article>

    <!-- Brief: AI buying cycle -->
    <article class="mt-12 border-b border-slate-200 pb-10">
      <time class="text-sm text-slate-500">February 22, 2026</time>
      <h2 class="text-xl font-semibold mt-1">Enterprise AI buying just got more complex — and that's actually good for Anthropic</h2>
      <div class="mt-3 text-slate-700 space-y-3">
        <p>
          The WSJ reported this week that selling specialized AI software has gotten harder. Companies are slowing down, bringing Finance and Legal into the room early, and scrutinizing ROI more carefully. The underlying worry: if frontier models keep advancing, will the specialized tool they buy today still be relevant in two years?
        </p>
        <p>
          It's a reasonable fear — and it's actually clarifying. Companies aren't buying <em>less</em> AI; they're buying <em>more carefully</em>. The purchases getting scrutinized are the specialized, point-solution tools. Frontier models sit in a different category: they're increasingly what makes those point solutions potentially obsolete.
        </p>
        <p>
          This is the environment I know well. As a CRO, I ran enterprise sales cycles that routinely ran 3 months to a year or more. That means building consensus across Legal (contracts, data governance, liability), Finance (ROI modeling, TCO, budget cycles), IT/Ops (security, integration, change management), and domain experts who actually have to use the thing. That's not a sequence — it all has to move at once, or deals stall.
        </p>
        <p>
          The conversations Anthropic is having now — especially with enterprise accounts weighing full Claude adoption vs. integrating Claude into existing systems vs. buying third-party AI tools — map directly to what I've navigated. I can help customers think through the macro question ("go all-in on Claude, integrate it selectively, or evaluate what to build vs. buy"), and I can run the room when Finance is asking about IRR and Legal is asking about data residency in the same meeting.
        </p>
        <p>
          <em>The buying cycle got more complex; navigating it well is increasingly what separates serious enterprise AI deployments from stalled ones.</em>
        </p>
      </div>
      <p class="mt-4 text-sm text-slate-500">
        Source: <a href="https://www.wsj.com" class="underline hover:text-slate-700">The Wall Street Journal</a> · Feb 22, 2026
      </p>
    </article>

    <!-- Brief: Workslop -->
    <article class="mt-12 border-b border-slate-200 pb-10">
      <time class="text-sm text-slate-500">February 19, 2026</time>
      <h2 class="text-xl font-semibold mt-1">AI mandates create "workslop" — fixing it requires system-level change</h2>
      <div class="mt-3 text-slate-700 space-y-3">
        <p>
          HBR introduced a useful term: <em>workslop</em> — the low-quality, AI-generated output that floods inboxes when organizations mandate AI use without thinking through the implications. The pressure is real: boards want leaner teams, execs feel the push to show AI ROI, and the implicit message is "do more with less."
        </p>
        <p>
          The authors argue this isn't a people problem — it's a system problem. Their three-level fix:
        </p>
        <ul class="list-disc list-outside ml-4 space-y-1">
          <li><strong>Culture:</strong> Rebuild trust through actual collaboration — feedback, questions, dialogue. Not just AI outputs.</li>
          <li><strong>Practice:</strong> Create clear norms for when/how to use AI, with review processes that reinforce human judgment rather than offload it.</li>
          <li><strong>Accountability:</strong> Someone needs to own the AI-human integration. The authors suggest "forward deployed AI collaboration architects" who understand both the tech and the people.</li>
        </ul>
        <p>
          <em>That last point caught my attention — it's basically describing a role focused on making AI actually work inside organizations.</em> The irony they point out: to make AI work at work, we need to get better at being human.
        </p>
      </div>
      <p class="mt-4 text-sm text-slate-500">
        Source: <a href="https://hbr.org/2026/01/why-people-create-ai-workslop-and-how-to-stop-it" class="underline hover:text-slate-700">Harvard Business Review</a> · Niederhoffer, Robichaux & Hancock · Jan 2026
      </p>
    </article>

    <!-- Brief: Hidden AI use -->
    <article class="mt-10 border-b border-slate-200 pb-10">
      <time class="text-sm text-slate-500">February 19, 2026</time>
      <h2 class="text-xl font-semibold mt-1">Employees are hiding their AI productivity gains — it's human nature</h2>
      <div class="mt-3 text-slate-700 space-y-3">
        <p>
          Ethan Mollick (Wharton) shared a striking finding on the Prof G podcast: about 50% of American workers are using AI and reporting 3x productivity gains on tasks where they use it. But here's the catch — many of them aren't telling their employers. They're not using corporate AI tools. They're keeping it quiet.
        </p>
        <p>
          Why? Because if you (and coworkers) prove you're 3x more efficient, you may be part of the next RIF. Often, the 'calculated' move is to pocket the gains and stay under the radar.
        </p>
        <p>
          This is a real problem, and it won't be solved with mandates or monitoring. It requires honesty: leaders need to make the case — credibly — that AI adoption benefits both the company <em>and</em> the employee. That's easier at growing companies ("we're expanding, let's make you more productive and reward you") than at struggling ones where a RIF feels inevitable.
        </p>
        <p>
          <em>The adoption gap isn't a training problem alone, it's a trust problem.</em>
        </p>
      </div>
      <p class="mt-4 text-sm text-slate-500">
        Source: <a href="https://www.profgmedia.com/podcast" class="underline hover:text-slate-700">Prof G Podcast</a> · Ethan Mollick (Wharton) · Feb 12, 2026
      </p>
    </article>

    <!-- Brief: Pro-worker AI -->
    <article class="mt-10 border-b border-slate-200 pb-10">
      <time class="text-sm text-slate-500">February 18, 2026</time>
      <h2 class="text-xl font-semibold mt-1">Pro-worker AI isn't automatic</h2>
      <div class="mt-3 text-slate-700 space-y-3">
        <p>
          MIT's Daron Acemoglu made a point this week that stuck with me: AI that actually helps workers requires deliberate design. It won't happen by default.
        </p>
        <p>
          Three things that resonated: build domain-specific systems aligned with how experts actually work, design for skill development (not just task completion), <strong>and add friction to prevent blind reliance on AI outputs.</strong>
        </p>
        <p>
          This maps to a best practice where you collaborate with AI tools rather than relying on them blindly. The best tools I've worked with don't try to replace judgment — they surface context and let the human decide. The worst ones optimize for "AI did the thing" without asking whether the thing was right.
        </p>
      </div>
      <p class="mt-4 text-sm text-slate-500">
        Source: <a href="https://mitsloan.mit.edu/ideas-made-to-matter/pro-worker-ai-doesnt-just-happen-companies-need-to-act" class="underline hover:text-slate-700">MIT Sloan</a>
      </p>
    </article>

  </main>

  <footer class="border-t">
    <div class="max-w-4xl mx-auto px-6 py-8 text-xs text-slate-500">
      &copy; <span id="y"></span> Tony Arteaga &bull; <a href="privacy.html" class="underline">Privacy</a>
    </div>
  </footer>
  <script>document.getElementById('y').textContent = new Date().getFullYear()</script>
</body>
</html>
